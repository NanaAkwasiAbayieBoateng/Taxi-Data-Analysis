---
title: "New York City Taxi and Limousine commission Data Analysis"
output: html_notebook
---

## Nana Akwasi Abayie Boateng

The Code below loads all the packages that would be required for this project.If a package is not available
in the library,it will be downloaded and loaded.

```{r}
#==================================================================
#  The follwing code :
# 1  Load all packages required for the analysis at the same time
# 2  Check if a required package is not already installed.
#3   If a required package is not already installed, the install thatt package
# 
#==================================================================

list=c("tidyverse","stringr","forcats","ggmap","rvest","tm","SnowballC","dplyr","calibrate","doParallel",
       "stringi","ggplot2","maps","httr","rsdmx","devtools","plyr","dplyr","ggplot2","caret","elasticnet",
       "magrittr","broom","glmnet","Hmisc",'knitr',"RSQLite","RANN","lubridate","ggvis","plotly","lars",
       "ggcorrplot","GGally","ROCR","lattice","car","corrgram","ggcorrplot","sqldf","parallel")

list_packages <- list
new.packages <- list_packages[!(list_packages %in% installed.packages()[,"Package"])]
if(length(new.packages)) install.packages(new.packages)

sapply(list, require, character.only = TRUE)


```

```{r}
#==================================================================
#  Set up parallel processing
# leave two cores for operating system
#==================================================================

cluster <- makeCluster(detectCores() - 2) 
registerDoParallel(cluster)

```



The  September 2015 data green taxi  contains 1048575  rows  and      21 columns.


```{r}
#==================================================================
# load the downloaded data with the readr package
#  dim function produces the dimension of the data
# the  September 2015 data contains 1048575  rows  and      21 columns
#==================================================================

data=readr::read_csv("C:/Users/Gucci148/Documents/DataMiningscience/Capitalone/green_tripdata_2015-09.csv")

names(data)

dim(data)
```










```{r}
#==================================================================
# Look at  the  structure of the data with the glimpse function in 
#  dplyr  package
#==================================================================

dplyr::glimpse(data)

summary(data)


```

```{r}
#==================================================================
# 
#  1 check missing obervations of each column variable
#  2 sum of all missing obervations in data
#  3   Ehail_fee  has all rows missing,remove from dataset
#==================================================================



apply(is.na.data.frame(data),2,sum)


newdata=dplyr::select(data,-Ehail_fee)

sum(is.na(newdata))
```



The average elapsed trip distance  reported by the taximeter is approximately 3 miles The median elapsed trip distance  reported by the taximeter is approximately 2 miles. The highest  elapsed trip distance  reported by the taximeter is approximately 603 miles and lowest is 0. The 603 mile appears to be an outlier. The data is skewed to the right with most observations are between zero and four.
```{r}
#==================================================================
# 
# Histogram of the Distribution of Trip Distance
# with ggplot2 package
#==================================================================

p<-ggplot(newdata, aes(x =Trip_distance )) + 
  geom_histogram(fill="black",col="black",alpha=0.2,binwidth=0.02) + 
theme_minimal() + ggtitle(" Histogram of the Distribution of Trip Distance") +
  #center title
  theme(plot.title = element_text(hjust = 0.5)) +xlab("Trip Distance")+ylab("Frequency")


ggplotly(p)
```



Right skewed distributions are not symmetric and transformations is usually done in order to build any model that uses a normal distribution assumption.Transformations improve symmetry,linear  relationships and constant variance which are usually need for models such as regression. A common transformation of right skewed data is the log transformation. 

```{r}

#==================================================================
# 
# Histogram of the Distribution of Log Trip Distance
# with ggplot2 package
#==================================================================

newdata=newdata%>%mutate(New_distance=log(Trip_distance+1)) 

p=ggplot(newdata, aes(x =New_distance ))+  geom_histogram(fill="black",col="black",binwidth=0.5) +
  theme_minimal() + ggtitle(" Histogram of the Distribution of Trip Distance") +
  #center title
  theme(plot.title = element_text(hjust = 0.5)) +xlab("Log of Trip Distance")+ylab("Frequency")

ggplotly(p)


```


The summary statistics of the transformed Trip distance shows the mean is about 3.027 and median is 2.010.


```{r}
newdata%>%mutate(Transform=log(Trip_distance+1))%>%dplyr::select(Trip_distance)%>%summary%>%knitr::kable()
```

The mean and median trip distance grouped by hour of day. The maximum mean  and median trip distance by hour 
 which is 4.192 and 2.95 respectively was travelled at fifth hour.

```{r}
#===========================================================================
# 
# Mean and median trip distance grouped by hour of day.
# Create Hours variable by extracting hours from the 
#  the  date and time when the meter was engaged (lpep_pickup_datetime).
#===========================================================================

newdata=newdata%>%dplyr::mutate(Hours= format(as.POSIXct(strptime(newdata$lpep_pickup_datetime,"%m/%d/%Y %H:%M",tz="")) ,format = "%H"))

newdata%>% mutate(Hours=as.numeric(Hours))%>%group_by(Hours)%>% dplyr::summarise(Mean=mean(Trip_distance),Median=median(Trip_distance),n=n())


```





```{r}
#===========================================================================
# Find the Hour in which  the  maximum mean  Trip distance occured
#===========================================================================

newdata%>% mutate(Hours=as.numeric(Hours))%>%group_by(Hours)%>% dplyr::summarise(Mean=mean(Trip_distance),Median=median(Trip_distance),n=n())%>%
  dplyr::slice(which.max(Mean )) 
```




```{r}

#===========================================================================
# Find the Hour in which  the  maximum  median  Trip distance occured
#===========================================================================

newdata%>% mutate(Hours=as.numeric(Hours))%>%group_by(Hours)%>% dplyr::summarise(Mean=mean(Trip_distance),Median=median(Trip_distance),n=n())%>%
  dplyr::slice(which.max(Median )) 
```





The new variable Tip is created by dividing Tip amount by the Total amount.The minimum Tip amount is -12.30. This appears to 
be an error in the data. The Tip amounts distribution is also right skewed  with most observations between 0 and 10. (99%).



```{r}
#==================================================================
# 
# derived variable for tip as a percentage of the total fare
#==================================================================


newdata=newdata%>%dplyr::mutate(Tip=(Tip_amount/Total_amount)*100)

length(newdata$Tip_amount[newdata$Tip_amount<10])/length(newdata$Tip_amount)
```

```{r}
#==================================================================
# 
# Histogram of the Distribution of Tip_amount
# with ggplot2 package
#==================================================================

p<-ggplot2::ggplot(newdata, aes(x =Tip_amount )) + 
  geom_histogram(fill="black",col="black",alpha=0.2,binwidth=0.02) + 
theme_minimal() + ggtitle(" Histogram of the Distribution of Tip amount") +
  #center title
  theme(plot.title = element_text(hjust = 0.5)) +xlab("Tip_amount")+ylab("Frequency")

ggplotly(p)


```

```{r}
# log transformation ,13.3 is added to prevent a case of taking log(0)

newdata=newdata%>%dplyr::mutate(New_Tip=log(Tip_amount+13.3))
                                       
```


The log transformation did not help out much in moving the data close to normal.We keep the old form in further analysis.
```{r}
p=ggplot2::ggplot(newdata, aes(x =New_Tip )) + 
  geom_histogram(fill="black",col="black",alpha=0.2,binwidth=0.2) + 
theme_minimal() + ggtitle(" Histogram of the Distribution of New Tip amount") +
  #center title
  theme(plot.title = element_text(hjust = 0.5)) +xlab("New Tip amount")+ylab("Frequency")

ggplotly(p)

```





```{r}

#==============================================================================
#   
# Building a Predictive Model
# Preprocessing data
# 1 There are three missing variables that will be removed at preprocessing step
# 2  Center and scale numerical columns to reduce variability
#==============================================================================


newdata=newdata[complete.cases(newdata),]

sum(is.na(newdata))


summary(newdata)

```




```{r}
#==================================================================
#Spliting training set into two parts based on outcome:
# Training set which 70% and Test set which 30% of the data
#==================================================================


index <- createDataPartition(newdata$Tip_amount, p=0.70, list=FALSE)

trainSet <- newdata[ index,]

testSet <- newdata[-index,]


sum(is.na(trainSet))
sum(is.na(testSet))
sum(is.na(newdata))

```


There are very  high positive  correlations between Total amount and Trip distance,of course which makes sense 
Total amount and Fare amount that passengers who travelled longer distances  were charged more.
There is also a significant positive correlation between Total amount and Tip amount.
Passengers who travelled longer distances and paid higher fares were more likely to tip.

```{r}
#==================================================================
#  Exploratory data analysis
# Determine correlation between variables
#==================================================================




newdata%>%dplyr::select(New_distance,Fare_amount,improvement_surcharge,Tolls_amount,Tip_amount,MTA_tax
                        ,Extra,Passenger_count,VendorID, Tip,Total_amount)%>%cor



```




```{r}
ggplot(trainSet , aes(Tip, Tip_amount))+geom_point(color="purple")+
  ggtitle('Tip vs Tip_amount')+
  xlab('Tip')+ylab('Tip_amount')+
  stat_smooth(method=lm, colour='black',span=0.2)
```
There exist a strong association between Tip and Tip amount.




```{r}
ggplot(trainSet , aes(Tip, Fare_amount))+geom_point(color="purple")+
  ggtitle('Tip vs Fare_amount')+
  xlab('Tip')+ylab('Fare_amount')+
  stat_smooth(method=lm, colour='black',span=0.2)
```





```{r}
ggplot(trainSet , aes(Tip, Trip_distance))+geom_point(color="purple")+
  ggtitle('Tip vs Trip_distance')+
  xlab('Tip')+ylab('Trip_distance')+
  stat_smooth(method=lm, colour='black',span=0.2)
```

The figure shows there are a few outlier points ,where the Tip was low for a very long Trip distance



```{r}
ggplot(trainSet , aes(Tip, New_distance))+geom_point(color="purple")+
  ggtitle('Tip vs  New_distance')+
  xlab('Tip')+ylab('New_distance')+
  stat_smooth(method=lm, colour='black',span=0.2)
```



```{r}
ggplot(trainSet , aes(Tip, Tolls_amount))+geom_point(color="purple")+
  ggtitle('Tip vs Tolls_amount')+
  xlab('Tip')+ylab('Tolls_amount')+
  stat_smooth(method=lm, colour='black',span=0.2)
```


 The Figure does reveal a weak positive association  between Tolls amount and Tip given.


```{r}
#==================================================================
#  Do feature selection using Generalized Linear Model
#==================================================================



control <- trainControl(method="repeatedcv", number=10, repeats=5)

my_glm<- train(trainSet[,c("Fare_amount","Trip_distance","Tolls_amount","MTA_tax","improvement_surcharge",
                          "Extra","Passenger_count","VendorID")], trainSet$Tip,
                    method = "glm",
                    preProc = c("center", "scale"),
                    trControl = control)

p=ggplot(varImp(my_glm, scale = FALSE))+ggtitle("Variable Importance using GLM")+theme_minimal()

ggplotly(p)
```

From the variable importance plot, we drop can drop improvement surcharge  since it has  have very 
little predictive power for Tip.


```{r}
#===============================================================================
#  Build a  Generalized Linear Model For with Stepwise Selection prediction
#===============================================================================

control <- trainControl(method="repeatedcv", number=10, repeats=5)


step_glm<- train(trainSet[,c("Fare_amount","Trip_distance","Tolls_amount","MTA_tax",
                          "Extra","Passenger_count","VendorID")], trainSet$Tip,
                    method = "glmStepAIC",
                    preProc = c("center", "scale"),
                    trControl = control)

```

```{r}
pred<-predict(step_glm,testSet[,c("Fare_amount","Trip_distance","Tolls_amount","MTA_tax",
                          "Extra","VendorID")])


glm_data=data_frame(predicted=pred,observed=testSet$Tip)

ggplot(glm_data,aes(predicted,observed))+geom_point()+geom_smooth(method=lm)+ggtitle('GLM')

```


```{r}
# Print, plot variable importance

print(varImp(my_glm, scale = FALSE))


```



```{r}
plot(varImp(my_glm, scale = FALSE), main="Variable Importance using GLM")


```


```{r}
# Mean Squared Error

sqrt(mean(pred- testSet$Tip)^2)
```




```{r}
#===============================================================================
#  Build a  Partial Least Squares Model 
#===============================================================================

# Partial Least Squares is one way to reduce dimension of the predictors used in the model. It identifies
#linear combinations,or directions, that best represent the predictors in the data.The directions are identified in 
# unsupervised way since the outcome variable is not in identifying the principal
# component directions.The predictors are preprocessed by centering and scaling.
# PLS will seek
#directions of maximum variation while simultaneously considering correlation with the response.


control <- trainControl(method="repeatedcv", number=10, repeats=5)

plsfit=train(trainSet[,c("Fare_amount","Trip_distance","Tolls_amount","MTA_tax",
                  "Extra","VendorID","Passenger_count")], trainSet$Tip,
      method = "glmStepAIC",tuneLength = 20,
      preProc = c("center", "scale"),
      trControl = control)






# Predict using test data


pred<-predict(plsfit,testSet[,c("Fare_amount","Trip_distance","Tolls_amount","MTA_tax",
                            "Extra","VendorID")])

my_data=data_frame(predicted=pred,observed=testSet$Tip)

ggplot(my_data,aes(predicted,observed))+geom_point()+geom_smooth(method=lm)+ggtitle('PLS')

```


```{r}
# Print, plot variable importance
print(varImp(plsfit, scale = FALSE))

```


```{r}
plot(varImp(plsfit, scale = FALSE), main="Variable Importance using PLS")
```


```{r}
# Mean Squared Error

sqrt(mean(pred- testSet$Tip)^2)
```




```{r}
#===============================================================================
#  Build Penalized linear regression models Model 
# Lasso (least absolute shrinkage and selection operator)
#===============================================================================

# The LASSO  penalizes the model  for having many predictors by shrinking the coefficients of 
# 
# those predictors with little variation to zero thereby reducing the dimension of the model.

lassoGrid <- expand.grid(.fraction = seq(.05, 1, length = 20))

control <- trainControl(method="repeatedcv", number=10, repeats=5)

set.seed(100)




lassofit=train(trainSet[,c("Fare_amount","Trip_distance","Tolls_amount","MTA_tax",
                  "Extra","VendorID")], trainSet$Tip,
      method = "lasso",tuneGrid = lassoGrid,
      preProc = c("center", "scale"),
      trControl = control)


# Predict using test data
pred<-predict(lassofit,testSet[,c("Fare_amount","Trip_distance","Tolls_amount","MTA_tax",
                                  "Extra","VendorID")])

my_data=data_frame(predicted=pred,observed=testSet$Tip)

ggplot(my_data,aes(predicted,observed))+geom_point()+geom_smooth(method=lm)+ggtitle('LASSO Model')

```

```{r}
print(varImp(lassofit, scale = FALSE), main="Variable Importance using lasso")
```


```{r}
plot(varImp(lassofit, scale = FALSE), main="Variable Importance using lasso")
```



```{r}
plot(lassofit)
```


```{r}
#Extract coefficients of final model

predict.enet(lassofit$finalModel, type='coefficients', s=lassofit$bestTune$fraction, mode='fraction')

```



```{r}
# Mean Squared Error

sqrt(mean(pred- testSet$Tip)^2)
```


I ran out memory after  several attempts to fit a random forest model.
```{r}
#===============================================================================
#  Random Forest linear regression models Model 
# 
#===============================================================================



control <- trainControl(method="repeatedcv", number=10, repeats=5)



rf_fit<-train(trainSet[,c("Fare_amount","Trip_distance","Tolls_amount","MTA_tax",
                  "Extra","VendorID")], trainSet$Tip,
      method = "rf", ntrees = 1000,importance = TRUE,
      preProc = c("center", "scale"),
      trControl = control)

# Predict using the test data

pred<-predict(rf_fit,testSet[,c("Fare_amount","Trip_distance","Tolls_amount","MTA_tax",
                                  "Extra","VendorID")])

my_data=adata_frame(predicted=pred,observed=testSet$Tip)

ggplot(my_data,aes(predicted,observed))+geom_point()+geom_smooth(method=lm)+ggtitle('Random Forest Model')

```


```{r}
plot(varImp(rf_fit, scale = FALSE),main="Variable Importance using RF")
```



```{r}
# Mean Squared Error

sqrt(mean(pred- testSet$Tip)^2)
```


I ran out memory after  several attempts to fit a gradient boosting machine.
```{r}
#===============================================================================
# Gradient Boosting Machine  Model 
# 
#===============================================================================

# To tune over interaction depth, number of trees, and shrinkage first define a tuning grid,
#then train over this grid


gbmGrid <- expand.grid(.interaction.depth = c(2,5,8),
                       .n.trees = c(500, 1000,2000,5000),
                       .shrinkage = c(0.01, 0.1),
                       .n.minobsinnode=c(5,10,15))

control <- trainControl(method="repeatedcv", number=10, repeats=5)


gbm_fit<-train(trainSet[,c("Fare_amount","Trip_distance","Tolls_amount","MTA_tax",
                    "Extra","VendorID")], trainSet$Tip,
        method = "gbm",verbose=F,tuneGrid = gbmGrid,
        preProc = c("center", "scale"),
        trControl = control)

# Predict using the test data

pred<-predict(gbm_fit,testSet[,c("Fare_amount","Trip_distance","Tolls_amount","MTA_tax",
                                "Extra","VendorID")])

my_data=adata_frame(predicted=pred,observed=testSet$Tip)


ggplot(my_data,aes(predicted,observed))+geom_point()+geom_smooth(method=lm)+ggtitle('Boosting')




```



```{r}
plot(varImp(gbm_fit, scale = FALSE),main="Variable Importance using Boosting")
```



```{r}
# Mean Squared Error

sqrt(mean(pred- testSet$Tip)^2)
```



We constructed a derived variable of time difference by subtracting pick up time from drop off time.The average speed of any 
trip was obtained by dividing distance covered by the time take. The time is converted to seconds to hours
which is standard  form.The Overall average speed off all trips is about 13 miles per hour.

```{r}
#Build a derived variable representing the average speed over the course of a trip.
#Average Speed=(distance)/time

# 1 create a TimeDifference variable by subtracting  lpep_pickup_datetime
# from Lpep_dropoff_datetime using the difftime function.
#2 Extract the  numeric values  in difference  in time(in seconds) by 
# by the stri_extract_all_regex function
# 3 unlist the result which is characer and convert to numeric
# 4  convert time  in seconds  to hours by multiplying by 0.000277778
# This standardizes speed in scientific units which is miles per hour
# 5 Time difference greater than zero seconds were selected in computing the 
#  speed to avoid undefined mathematical expressions
# 6 Speed variable=Trip distance/ TimeDifference

# Average speed=mean of the Speed variable

# The Average speed over all of september is approximately 5.844  meters per second


newdata=newdata%>%mutate(TimeDifference=
              as.numeric(unlist( stri_extract_all_regex(difftime(as.POSIXct(Lpep_dropoff_datetime, format="%m/%d/%Y %H:%M", tz="")
                                                                 , as.POSIXct(lpep_pickup_datetime, format="%m/%d/%Y %H:%M", tz="")
                                                                 , tz="",units = , "secs"), "[0-9]+"))))%>%mutate(Time=TimeDifference*0.000277778)



newdata=newdata[newdata$Time>0,]



#newdata=newdata%>%mutate(Speed=Trip_distance/TimeDifference)%>%dplyr::filter[newdata,TimeDifference >0]

# The filter approach seems not to recognise TimeDifference variable just created.

newdata=newdata%>%mutate(Speed=Trip_distance/Time)




```



The overall average speed for all  trips by green taxis is found to be 13.06 miles per hour.

```{r}
newdata%>%dplyr::summarise(Average_Speed=mean(Speed))

```

 The average trip speeds are materially different  in all weeks of September.We tested for the difference in 
 means between the three weeks in September per the dataset by an using Kruskal-Wallis and ANOVA model.Since the trip distance is right skewed,we proceed further to use a non-parametric test like Kruskal-Wallis to  test the differences between the speeds over the three weeks in September.Both parametric Anova without any transformations gives a similar outcome 
 like the non-parametric Kruskal-Wallis test.We found thet  there is  asignificant difference in the average Speed  by week travelled in September(p-value  < 2.2e-16).The first week had the highest average speed of about 13.75.
 Labor weekend occurred in the  first week of Septtember 2015,that could have contributed to the
 Higher speed in the first week of September as a lot of people were in a rush to get to their destinations
 For the holidays.

```{r}
#  Test of the mean and Median of average trip speeds
# in all weeks of September
# 1 Create a derived variable Week for the number of weeks in Septemeber
#  present in the data
# 2  We then group speed by week 
# 3 Test for difference in Mean speed for the weeks in September.
# 4 We will perform Kruskal Wallis nonparametric test and compare with parametric ANOVA 
#test of  the mean Speeds in the 3 weeks in September. The ANOVA test assumes the data is
# normally distributed wheras Kruskal-Wallis test does not.

week1=c("09/01/2015","09/02/2015","09/03/2015","09/04/2015","09/05/2015","09/06/2015","09/07/2015")
week2=c("09/08/2015","09/09/2015","09/10/2015","09/11/2015","09/12/2015","09/13/2015","09/14/2015")
week3=c("09/15/2015","09/16/2015","09/17/2015","09/18/2015","09/19/2015","09/20/2015","09/21/2015")

newdata=newdata%>%
  dplyr::mutate(dates= format(as.POSIXct(strptime(newdata$lpep_pickup_datetime,"%m/%d/%Y %H:%M",tz="")) ,format = "%m/%d/%Y"))

newdata=newdata %>%  
  mutate(Week = case_when(.$dates %in% week1 ~ 1, .$dates %in% week2 ~ 2,.$dates %in% week3 ~ 3)) 

newdata
```



```{r}
# Find the mean speed in each in September

newdata %>%group_by(Week)%>% dplyr::summarise(mean=mean(Speed))
 
```


```{r}
newdata %>%dplyr::select( Week,Speed)%>%mutate(Week=factor(Week))%>%kruskal.test()%>%tidy()
```



```{r}
newdata %>%dplyr::select( Week,Speed)%>%lm(formula = Speed ~ factor(Week ))%>%anova()%>%tidy()

```



A plausible Hypothesis would be to test if speed is associated with time of the day.This can be done by building a Linear regression model.
 There exist a significant association between Speed and Hour of the day. The coefficient of Hours is negative which indicates a negative relationship between the two variables. For every unit increase in Hour of the day,the 
 Speed decreases by a factor of about 0.106343.


```{r}
ggplot(newdata, aes(Hours, Speed))+geom_point(color="purple")+
  ggtitle('Hours vs Speed')+
  xlab('Hours')+ylab('Speed')+
  stat_smooth(method=lm, colour='black',span=0.2)

```


```{r}
newdata %>%dplyr::select(Speed,Hours)%>%mutate( Hours=as.numeric( Hours))%>%
  lm(formula = Speed ~ (Hours ))%>%summary()


```


```{r}
fit_glm=glm( Speed ~ as.numeric(Hours), data=newdata)


tidy(fit_glm)
```


```{r}
head(augment(fit_glm))
```



```{r}
glance(fit_glm)
```





Other insights one would consider given ample time include the following:

1. Spatial data analysis with get_map and ggmap to determine if the green taxis followed the citys order to pick up 
paasengers from Manhattan eclusionary zone.
2. Building predictive models with deep neural networks which is known to have better predictive power.

3. Showing interactive plots to assist in detecting any underlying patterns of the data.